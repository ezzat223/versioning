# =============================================================================
# Ray Serve Dockerfile (Official Base Image)
# =============================================================================
# Use Official Ray Image
FROM rayproject/ray:2.9.0-py310

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY src/ ./src/

# Build-time arguments (optional overrides)
ARG MODEL_NAME=model-name
ARG MODEL_VERSION=latest
ARG MLFLOW_TRACKING_URI=http://127.0.0.1:5001
ARG SERVE_MIN_REPLICAS=1
ARG SERVE_MAX_REPLICAS=4
ARG SERVE_TARGET_CONCURRENCY=8
ARG SERVE_NUM_CPUS_PER_REPLICA=1

# Environment Variables
ENV MODEL_NAME=${MODEL_NAME} \
    MODEL_VERSION=${MODEL_VERSION} \
    MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI} \
    SERVE_MIN_REPLICAS=${SERVE_MIN_REPLICAS} \
    SERVE_MAX_REPLICAS=${SERVE_MAX_REPLICAS} \
    SERVE_TARGET_CONCURRENCY=${SERVE_TARGET_CONCURRENCY} \
    SERVE_NUM_CPUS_PER_REPLICA=${SERVE_NUM_CPUS_PER_REPLICA} \
    PYTHONUNBUFFERED=1

# Expose Ray Serve Port
EXPOSE 8000

# Start Ray Serve
# This one is for real-time inference
# Replace ray_serve with ray_batch for batch inference
CMD ["serve", "run", "src.deployment.ray_serve:entrypoint"]
