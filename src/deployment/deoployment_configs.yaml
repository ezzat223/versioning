# =============================================================================
# Deployment Configurations for Multiple Platforms
# =============================================================================
# Choose the appropriate config based on your deployment target

# =============================================================================
# AWS LAMBDA (BentoML + bentoctl)
# =============================================================================
aws_lambda:
  api_version: v1
  name: iris-classifier-lambda
  operator:
    name: aws-lambda
  template: terraform
  spec:
    region: us-west-2
    timeout: 60
    memory_size: 3008  # MB
    deployment_type: lambda
    enable_xray: true
    environment_variables:
      MLFLOW_TRACKING_URI: "https://your-mlflow-server.com"
      MODEL_NAME: "iris-classifier"
      MODEL_VERSION: "latest"

# Usage:
# bentoctl init
# bentoctl build -b iris-classifier-service:latest -f deployment/deployment_configs.yaml:aws_lambda
# bentoctl deploy -b iris-classifier-service:latest

# =============================================================================
# GCP CLOUD RUN (BentoML + bentoctl)
# =============================================================================
gcp_cloud_run:
  api_version: v1
  name: iris-classifier-cloudrun
  operator:
    name: google-cloud-run
  template: terraform
  spec:
    project_id: your-gcp-project
    region: us-central1
    timeout: 300
    port: 3000
    min_instances: 0
    max_instances: 10
    memory: "2Gi"
    cpu: "2"
    environment_variables:
      MLFLOW_TRACKING_URI: "https://your-mlflow-server.com"
      MODEL_NAME: "iris-classifier"
      MODEL_VERSION: "latest"

# Usage:
# bentoctl init
# bentoctl build -b iris-classifier-service:latest -f deployment/deployment_configs.yaml:gcp_cloud_run
# bentoctl deploy -b iris-classifier-service:latest

# =============================================================================
# AZURE CONTAINER INSTANCES (BentoML + bentoctl)
# =============================================================================
azure_container_instances:
  api_version: v1
  name: iris-classifier-aci
  operator:
    name: azure-container-instances
  template: terraform
  spec:
    location: eastus
    cpu: 2
    memory: 4
    port: 3000
    environment_variables:
      MLFLOW_TRACKING_URI: "https://your-mlflow-server.com"
      MODEL_NAME: "iris-classifier"
      MODEL_VERSION: "latest"

# =============================================================================
# RAY SERVE ON KUBERNETES
# =============================================================================
ray_serve_k8s:
  apiVersion: v1
  kind: Service
  metadata:
    name: ray-serve-service
    namespace: ml-serving
  spec:
    type: LoadBalancer
    selector:
      app: ray-serve
    ports:
      - protocol: TCP
        port: 8000
        targetPort: 8000
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: ray-serve-deployment
    namespace: ml-serving
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: ray-serve
    template:
      metadata:
        labels:
          app: ray-serve
      spec:
        containers:
          - name: ray-serve
            image: your-registry/iris-classifier-ray:latest
            ports:
              - containerPort: 8000
            env:
              - name: MODEL_NAME
                value: "iris-classifier"
              - name: MODEL_VERSION
                value: "latest"
              - name: NUM_REPLICAS
                value: "2"
            resources:
              requests:
                memory: "2Gi"
                cpu: "1"
              limits:
                memory: "4Gi"
                cpu: "2"

# =============================================================================
# RAY CLUSTER ON EC2/GCP/AZURE
# =============================================================================
ray_cluster_config:
  cluster_name: ml-inference-cluster
  max_workers: 10
  upscaling_speed: 1.0
  idle_timeout_minutes: 5

  provider:
    type: aws  # or 'gcp' or 'azure'
    region: us-west-2
    availability_zone: us-west-2a

  auth:
    ssh_user: ubuntu

  head_node:
    InstanceType: m5.xlarge
    ImageId: ami-0abcdef1234567890  # Deep Learning AMI

  worker_nodes:
    InstanceType: m5.large
    ImageId: ami-0abcdef1234567890
    MinWorkers: 2
    MaxWorkers: 10

  setup_commands:
    - pip install ray[default] mlflow pandas scikit-learn xgboost

  head_start_ray_commands:
    - ray stop
    - ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265

  worker_start_ray_commands:
    - ray stop
    - ray start --address=$RAY_HEAD_IP:6379

# Usage:
# ray up deployment/ray_cluster_config.yaml
# ray attach deployment/ray_cluster_config.yaml
# python src/deployment/ray_serve_online.py
# ray down deployment/ray_cluster_config.yaml

# =============================================================================
# DOCKER COMPOSE (LOCAL/ON-PREM)
# =============================================================================
docker_compose:
  version: '3.8'

  services:
    # Ray Serve for online inference
    ray-serve:
      image: your-registry/iris-classifier-ray:latest
      ports:
        - "8000:8000"
        - "8265:8265"  # Ray dashboard
      environment:
        - MODEL_NAME=iris-classifier
        - MODEL_VERSION=latest
        - MLFLOW_TRACKING_URI=http://mlflow:5001
        - NUM_REPLICAS=2
      volumes:
        - ./models:/app/models
      deploy:
        replicas: 1
        resources:
          limits:
            cpus: '2'
            memory: 4G
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8000/"]
        interval: 30s
        timeout: 10s
        retries: 3

    # BentoML for serverless-style deployment
    bentoml-service:
      image: your-registry/iris-classifier-bento:latest
      ports:
        - "3000:3000"
      environment:
        - MODEL_NAME=iris-classifier
        - MODEL_VERSION=latest
        - MLFLOW_TRACKING_URI=http://mlflow:5001
      volumes:
        - ./models:/app/models
      deploy:
        replicas: 2
        resources:
          limits:
            cpus: '1'
            memory: 2G

    # MLflow (if not external)
    mlflow:
      image: mlflow/mlflow:latest
      ports:
        - "5001:5001"
      command: mlflow server --host 0.0.0.0 --port 5001
      volumes:
        - ./mlruns:/mlruns

    # Nginx load balancer (optional)
    nginx:
      image: nginx:alpine
      ports:
        - "80:80"
      volumes:
        - ./nginx.conf:/etc/nginx/nginx.conf:ro
      depends_on:
        - ray-serve
        - bentoml-service

# Usage:
# docker-compose -f deployment/docker-compose.yaml up -d
# docker-compose -f deployment/docker-compose.yaml ps
# docker-compose -f deployment/docker-compose.yaml logs -f
# docker-compose -f deployment/docker-compose.yaml down
