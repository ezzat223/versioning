"""
Generate CML (Continuous Machine Learning) report for GitLab comments.
Creates visual comparison reports between challenger and champion models.
"""
import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import Dict


class CMLReportGenerator:
    """Generate markdown reports for CML."""

    def __init__(self, comparison_report: Dict):
        """
        Initialize CML report generator.

        Args:
            comparison_report: Model comparison report from compare_models.py
        """
        self.report = comparison_report

    def _format_metric(self, value: float, precision: int = 4) -> str:
        """Format metric value."""
        if isinstance(value, (int, float)):
            return f"{value:.{precision}f}"
        return str(value)

    def _create_header(self) -> str:
        """Create report header."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        header = f"""# ğŸ¤– Model Comparison Report

**Generated:** {timestamp}
**Experiment:** `{self.report.get('experiment_name', 'N/A')}`

---

"""
        return header

    def _create_decision_section(self) -> str:
        """Create decision section with emoji indicators."""
        should_promote = self.report.get('promote_challenger', False)
        comparison = self.report.get('comparison', {})
        reason = comparison.get('reason', 'No reason provided')

        if should_promote:
            emoji = "âœ…"
            decision = "PROMOTE CHALLENGER"
            color = "ğŸŸ¢"
        else:
            emoji = "âš ï¸"
            decision = "KEEP CHAMPION"
            color = "ğŸŸ¡"

        section = f"""## {emoji} Decision: {decision}

{color} **Reason:** {reason}

---

"""
        return section

    def _create_metrics_comparison_table(self) -> str:
        """Create metrics comparison table."""
        comparison = self.report.get('comparison', {})

        if not comparison:
            return "No comparison data available.\n\n"

        champion_metrics = comparison.get('all_champion_metrics', {})
        challenger_metrics = comparison.get('all_challenger_metrics', {})

        # If no champion, show only challenger
        if not champion_metrics:
            section = """## ğŸ“Š Challenger Metrics

| Metric | Value |
|--------|-------|
"""
            for metric, value in challenger_metrics.items():
                section += f"| {metric} | {self._format_metric(value)} |\n"

            section += "\n> **Note:** No existing champion model - this is the first model.\n\n"
            return section

        # Full comparison table
        section = """## ğŸ“Š Metrics Comparison

| Metric | Champion | Challenger | Î” | Î”% |
|--------|----------|------------|---|-----|
"""

        # Combine all metrics
        all_metrics = set(champion_metrics.keys()) | set(challenger_metrics.keys())

        for metric in sorted(all_metrics):
            champ_val = champion_metrics.get(metric, 0)
            chall_val = challenger_metrics.get(metric, 0)

            if champ_val > 0:
                delta = chall_val - champ_val
                delta_pct = (delta / champ_val) * 100

                # Add emoji indicator
                if delta > 0:
                    indicator = "ğŸ“ˆ"
                elif delta < 0:
                    indicator = "ğŸ“‰"
                else:
                    indicator = "â¡ï¸"

                section += (
                    f"| {metric} | {self._format_metric(champ_val)} | "
                    f"{self._format_metric(chall_val)} | "
                    f"{indicator} {self._format_metric(delta, 6)} | "
                    f"{delta_pct:+.2f}% |\n"
                )
            else:
                section += (
                    f"| {metric} | {self._format_metric(champ_val)} | "
                    f"{self._format_metric(chall_val)} | - | - |\n"
                )

        section += "\n"
        return section

    def _create_improvement_section(self) -> str:
        """Create improvement details section."""
        comparison = self.report.get('comparison', {})

        if not comparison or not comparison.get('champion_score'):
            return ""

        primary_metric = comparison.get('primary_metric', 'test_accuracy')
        champ_score = comparison.get('champion_score', 0)
        chall_score = comparison.get('challenger_score', 0)
        improvement = comparison.get('improvement', 0)
        improvement_pct = comparison.get('improvement_pct', 0)
        threshold = comparison.get('threshold', 0.01)
        threshold_pct = comparison.get('threshold_pct', 1.0)

        section = f"""## ğŸ¯ Primary Metric: `{primary_metric}`

| | Score | |
|---|---|---|
| **Champion** | {self._format_metric(champ_score)} | |
| **Challenger** | {self._format_metric(chall_score)} | |
| **Improvement** | {self._format_metric(improvement, 6)} | **{improvement_pct:+.2f}%** |
| **Threshold** | {self._format_metric(threshold, 6)} | {threshold_pct:.2f}% |

"""

        # Visual progress bar
        if improvement >= threshold:
            bar = "ğŸŸ©" * 10
            status = "âœ… Above threshold"
        else:
            filled = int((improvement / threshold) * 10) if threshold > 0 else 0
            filled = max(0, min(10, filled))
            bar = "ğŸŸ©" * filled + "â¬œ" * (10 - filled)
            status = "âš ï¸ Below threshold"

        section += f"""**Progress:** {bar} {status}

---

"""
        return section

    def _create_run_info_section(self) -> str:
        """Create run information section."""
        champion_id = self.report.get('champion_run_id', 'None')
        challenger_id = self.report.get('challenger_run_id', 'N/A')

        section = f"""## ğŸ”— Run Information

| Role | Run ID |
|------|--------|
| Champion | `{champion_id}` |
| Challenger | `{challenger_id}` |

---

"""
        return section

    def _create_next_steps_section(self) -> str:
        """Create next steps section."""
        should_promote = self.report.get('promote_challenger', False)

        if should_promote:
            section = """## ğŸš€ Next Steps

The CI/CD pipeline will automatically:

1. âœ… Promote challenger to champion
2. ğŸ“¦ Archive old champion model
3. ğŸ³ Build and push Docker image
4. ğŸ·ï¸ Create git release tag

**Action Required:** None - automation will handle promotion.

"""
        else:
            section = """## ğŸ”„ Next Steps

The challenger did not meet the promotion criteria.

**Recommendations:**
- Review the metrics and identify improvement areas
- Consider hyperparameter tuning
- Add more training data
- Try different model architectures

**Action Required:** None - current champion remains in production.

"""
        return section

    def generate_report(self) -> str:
        """
        Generate complete CML report.

        Returns:
            Markdown report string
        """
        report_parts = [
            self._create_header(),
            self._create_decision_section(),
            self._create_improvement_section(),
            self._create_metrics_comparison_table(),
            self._create_run_info_section(),
            self._create_next_steps_section()
        ]

        # Add footer
        report_parts.append("""---

*This report was automatically generated by the MLOps CI/CD pipeline.*

**Legend:**
- ğŸ“ˆ Improvement
- ğŸ“‰ Regression
- â¡ï¸ No change
- ğŸŸ¢ Passed
- ğŸŸ¡ Below threshold
""")

        return "".join(report_parts)


def main():
    parser = argparse.ArgumentParser(description="Generate CML report for model comparison")
    parser.add_argument(
        "--comparison",
        type=str,
        required=True,
        help="Path to comparison report JSON"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="cml_report.md",
        help="Output markdown file"
    )

    args = parser.parse_args()

    # Load comparison report
    with open(args.comparison, 'r') as f:
        comparison_report = json.load(f)

    # Generate report
    generator = CMLReportGenerator(comparison_report)
    markdown_report = generator.generate_report()

    # Save report
    output_path = Path(args.output)
    output_path.write_text(markdown_report)

    print(f"âœ“ CML report generated: {args.output}")
    print("\n" + "="*70)
    print("REPORT PREVIEW")
    print("="*70)
    print(markdown_report)


if __name__ == "__main__":
    main()
