stages:
  train_supervised:
    cmd: >
      python src/train.py
      --data-path ${data.path}
      --target-column ${supervised.target_column}
      --test-size ${data.test_size}
      --validation-size ${data.validation_size}
      --random-state ${data.random_state}
      --n-estimators ${supervised.model.n_estimators}
      --max-depth ${supervised.model.max_depth}
      --experiment-name ${mlflow.experiment_name}
      --model-name ${mlflow.model_name}
      --strict-git false
    deps:
      - src/train.py
      - src/data_loaders/
      - src/utils.py
      - ${data.path}
    params:
      - data.path
      - data.test_size
      - data.validation_size
      - data.random_state
      - supervised.target_column
      - supervised.model.n_estimators
      - supervised.model.max_depth
      - mlflow.experiment_name
      - mlflow.model_name
    metrics:
      - metrics.json:
          cache: false


# parameters
#   dvc exp run --queue -S supervised.model.n_estimators=50
#   dvc exp run --queue -S supervised.model.n_estimators=100
#   dvc exp run --queue -S supervised.model.n_estimators=200
#   dvc exp run --queue -S supervised.model.max_depth=5
#   dvc exp run --queue -S supervised.model.max_depth=10
#   dvc exp run --queue -S supervised.model.max_depth=15
#
# Step 2: Run all queued experiments
#   dvc queue start
#
# Step 3: View results
#   dvc exp show
#   dvc exp show --no-pager | head -20
#
# GRID SEARCH EXAMPLE:
#   # Queue all combinations
#   for n_est in 50 100 200; do
#     for max_d in 5 10 15; do
#       dvc exp run --queue -S supervised.model.n_estimators=$n_est -S supervised.model.max_depth=$max_d
#     done
#   done
#   
#   # Run all at once
#   dvc queue start
#   
#   # Or run in parallel (4 jobs)
#   dvc queue start --jobs 4
#
#
# APPROACH 2: RUN WITH COMMAND LINE ARGUMENTS (Override params.yaml)
# ------------------------------------------------------------------------------
# Override parameters from command line without editing params.yaml
#
# Single parameter override:
#   dvc exp run -S supervised.model.n_estimators=150
#
# Multiple parameter overrides:
#   dvc exp run -S supervised.model.n_estimators=150 -S supervised.model.max_depth=20
#
# Give experiment a name:
#   dvc exp run -n "experiment-high-estimators" -S supervised.model.n_estimators=300
#
# Force re-run (ignore cache):
#   dvc exp run -f -S supervised.model.n_estimators=150
#
# =============================================================================
# EXPERIMENT MANAGEMENT COMMANDS
# =============================================================================
#
# QUEUE MANAGEMENT:
#   dvc queue start                # Run all queued experiments
#   dvc queue start --jobs 4       # Run 4 experiments in parallel
#   dvc queue logs                 # View logs from queued experiments
#   dvc queue remove --all         # Clear the queue
#   dvc queue status               # Check queue status
#
# VIEWING EXPERIMENTS:
#   dvc exp show                   # Show all experiments in table
#   dvc exp show --no-pager        # Show without pager (better for piping)
#   dvc exp show --only-changed    # Show only changed metrics/params
#   dvc exp show --include-params  # Include all parameters
#   dvc exp diff                   # Compare current vs baseline
#   dvc exp diff exp-name          # Compare specific experiment
#
# MANAGING EXPERIMENTS:
#   dvc exp apply exp-name         # Apply experiment to workspace
#   dvc exp branch exp-name my-branch  # Convert experiment to git branch
#   dvc exp remove exp-name        # Delete experiment
#   dvc exp gc --workspace         # Clean up old experiments
#
# COMPARING EXPERIMENTS:
#   dvc plots diff                 # Compare plots across experiments
#   dvc metrics diff               # Compare metrics across experiments
#   dvc params diff                # Compare parameters across experiments
#
#
# =============================================================================
# TYPICAL WORKFLOWS
# =============================================================================
#
# WORKFLOW 1: QUICK SINGLE EXPERIMENT
#   # Override one parameter and run
#   dvc exp run -S supervised.model.n_estimators=200
#   
#   # View result
#   dvc exp show
#
#
# WORKFLOW 2: HYPERPARAMETER GRID SEARCH
#   # Queue all combinations
#   for n in 50 100 150 200; do
#     for d in 5 10 15 20; do
#       dvc exp run --queue -S supervised.model.n_estimators=$n -S supervised.model.max_depth=$d
#     done
#   done
#   
#   # Run all (or run in parallel with --jobs)
#   dvc queue start --jobs 4
#   
#   # Find best experiment
#   dvc exp show --sort-by test_accuracy --sort-order desc
#   
#   # Apply best experiment to workspace
#   dvc exp apply best-exp-name
#   
#   # Commit it
#   git add params.yaml dvc.lock metrics.json
#   git commit -m "Best model: n_estimators=150, max_depth=10, accuracy=0.95"
#
#
# WORKFLOW 3: INCREMENTAL EXPERIMENTATION
#   # Try different values iteratively
#   dvc exp run -n "baseline" 
#   dvc exp run -n "more-trees" -S supervised.model.n_estimators=200
#   dvc exp run -n "deeper-trees" -S supervised.model.max_depth=20
#   
#   # Compare all
#   dvc exp show
#   
#   # Pick winner and apply
#   dvc exp apply more-trees
#
#
# WORKFLOW 4: COMPARING WITH BASELINE
#   # Run new experiment
#   dvc exp run -n "new-approach" -S supervised.model.n_estimators=300
#   
#   # Compare with current main branch
#   dvc exp diff
#   
#   # If better, apply and commit
#   dvc exp apply new-approach
#   git add params.yaml dvc.lock metrics.json
#   git commit -m "Improved accuracy by 2% with 300 estimators"
#
#
# =============================================================================
# RUNNING IN DIFFERENT ENVIRONMENTS
# =============================================================================
#
# LOCAL DEVELOPMENT:
#   conda activate your-env-name
#   dvc repro
#
# CI/CD PIPELINE:
#   # .github/workflows/train.yml or similar
#   steps:
#     - uses: actions/checkout@v2
#     - name: Setup Python
#       uses: actions/setup-python@v2
#     - name: Install dependencies
#       run: |
#         conda env create -f environment.yaml
#         conda activate your-env-name
#     - name: Run training
#       run: dvc repro
#
# JUPYTER NOTEBOOK:
#   # In a notebook cell:
#   !dvc exp run -S supervised.model.n_estimators=150
#
# REMOTE EXECUTION (e.g., on a server):
#   ssh your-server
#   cd /path/to/project
#   conda activate your-env-name
#   dvc exp run --queue -S supervised.model.n_estimators=200
#   dvc queue start
#
#
# =============================================================================
# BEST PRACTICES
# =============================================================================
#
# 1. ALWAYS ACTIVATE YOUR CONDA ENVIRONMENT FIRST
#    conda activate your-env-name
#    # Then run dvc commands
#
# 2. USE EXPERIMENT QUEUE FOR MULTIPLE RUNS
#    # Better than running sequentially
#    dvc exp run --queue -S param1=value1
#    dvc exp run --queue -S param2=value2
#    dvc queue start --jobs 4  # Parallel execution
#
# 3. NAME YOUR EXPERIMENTS
#    dvc exp run -n "descriptive-name" -S param=value
#
# 4. COMMIT WINNING EXPERIMENTS
#    dvc exp apply best-exp
#    git add params.yaml dvc.lock metrics.json
#    git commit -m "Best model: accuracy=0.95"
#
# 5. MODELS GO TO MLFLOW, DATA GOES TO DVC
#    - Models: MLflow Model Registry (automatic)
#    - Data: DVC remote storage (dvc push)
#    - Metrics: Both DVC (metrics.json) and MLflow
#
# 6. USE DVC FOR REPRODUCIBILITY, MLFLOW FOR MODEL TRACKING
#    - DVC: Tracks what parameters produced what metrics
#    - MLflow: Tracks model artifacts, compares runs in UI
#    - Together: Complete ML experiment tracking
#
#
# =============================================================================
# COMMON DVC COMMANDS QUICK REFERENCE
# =============================================================================
#
# BASIC PIPELINE:
#   dvc repro                      # Run pipeline with params.yaml values
#   dvc repro -f                   # Force re-run (ignore cache)
#   dvc dag                        # Show pipeline DAG
#   dvc status                     # Check for changes
#
# EXPERIMENTS:
#   dvc exp run                    # Run experiment (same as dvc repro)
#   dvc exp run -S param=value     # Override parameter
#   dvc exp run -n "name"          # Name your experiment
#   dvc exp show                   # Show all experiments
#   dvc exp diff                   # Compare with baseline
#
# QUEUE:
#   dvc exp run --queue            # Add to queue
#   dvc queue start                # Run queue
#   dvc queue start --jobs 4       # Parallel execution
#
# METRICS & PARAMS:
#   dvc metrics show               # Show current metrics
#   dvc metrics diff               # Compare metrics across commits
#   dvc params diff                # Compare parameters across commits
#
# DATA MANAGEMENT:
#   dvc push                       # Push data to remote storage
#   dvc pull                       # Pull data from remote storage
#   dvc fetch                      # Download data without checkout
#
#
# =============================================================================
# NOTES
# =============================================================================
#
# WHY NO MLFLOW PROJECTS HERE?
#   - MLflow projects create isolated environments (slow)
#   - We want to use our existing conda environment (fast)
#   - Direct Python execution is simpler and more transparent
#   - MLflow still tracks runs through mlflow.start_run() in train.py
#
# WHY NO MODEL OUTPUTS IN DVC?
#   - Models are large binary files
#   - MLflow Model Registry handles model versioning better
#   - MLflow provides model serving, staging (dev/prod), lineage
#   - DVC focuses on data versioning and pipeline reproducibility
#   - Together they provide complete MLOps coverage
#
# WHAT DOES DVC TRACK?
#   - Pipeline dependencies (code, data files)
#   - Parameters (params.yaml)
#   - Metrics (metrics.json)
#   - Experiment history and comparisons
#   - Data files (with dvc add / dvc push)
#
# WHAT DOES MLFLOW TRACK?
#   - Model artifacts and registry
#   - Run parameters and metrics (same as DVC but in UI)
#   - Model lineage and staging
#   - Experiment comparison in web UI
#   - Model serving and deployment
